#!/usr/bin/env python3
"""
QUANTUM EMBODIMENT QUEST
A Scientific-Grade Fun Game

Merge of ApexPazuzu Nexus Core v7.0 and AGI Sandbox
- Quantum-accurate mechanics with educational tooltips
- Embodied AGI exploration in procedural quantum-voxel worlds
- 24 unlockable embodiment approaches
- Win condition: Stabilize 10 timelines
- Lose condition: Resource exhaustion or infinite fall

Controls: move [north|south|east|west|up|down], interact, jump, rest, fork, upgrade
"""

import numpy as np
import random
import time
import sys
import math
from typing import Dict, Tuple, List, Optional, Any
from collections import defaultdict, deque
from enum import Enum

# === CONFIGURATION ===
class Config:
    # World
    CHUNK_SIZE = 16
    CHUNK_HEIGHT = 128
    TERRAIN_BASE_HEIGHT = 60
    HILL_AMPLITUDE = 10
    MOUNTAIN_AMPLITUDE = 25
    VIEW_DISTANCE = 3
    
    # Physics
    GRAVITY = -9.81
    VOXEL_MASS = 1.0
    MAX_VELOCITY = 50.0
    COLLISION_SUB_STEPS = 4
    
    # Quantum
    QUANTUM_ENTROPY_THRESHOLD = 0.5
    ENTANGLEMENT_RANGE = 8.0
    SUPERPOSITION_DECOHERENCE_RATE = 0.01
    
    # Game
    MAX_TIMELINES = 10
    QUANTUM_ORES_TO_WIN = 10
    MAX_FALL_DISTANCE = -1000
    EMBODIMENT_UNLOCK_COST = 100
    
    # Cognitive
    WORKING_MEMORY_LIMIT = 7  # Miller's Law
    CURIOSITY_BONUS = 0.1
    FATIGUE_THRESHOLD = 0.8

config = Config()

# === LOGGING WITH PERSONALITY ===
class Logger:
    def __init__(self):
        self.quirky_messages = [
            "Oh no, your superposition just ghosted you!",
            "Coherence lowâ€”time for a quantum coffee break!",
            "Your wave function is looking a bit shaky!",
            "Entanglement achieved! Now you're really connected!",
            "Quantum ore detected! SchrÃ¶dinger's loot box awaits!",
            "Reality forked! Now you're living the multiverse dream!",
            "Haptic senses tinglingâ€”quantum vibrations detected!",
            "Inertial drift engaged! Who needs walking anyway?",
            "Curiosity drive activated! The universe wonders with you!",
            "Eco-feedback received! The world is responding to your presence!"
        ]
    
    def info(self, msg): 
        print(f"[INFO] {msg}")
    
    def warning(self, msg): 
        print(f"[WARN] {msg}")
    
    def error(self, msg): 
        print(f"[ERROR] {msg}")
    
    def critical(self, msg): 
        print(f"[CRIT] {msg}")
    
    def quirky(self):
        print(f"[QUIRKY] {random.choice(self.quirky_messages)}")

logger = Logger()

# === CORE DATA STRUCTURES ===
class BlockType(Enum):
    AIR = 0
    STONE = 1
    DIRT = 2
    GRASS = 3
    WATER = 5
    QUANTUM_ORE = 6
    BEDROCK = 7

class Chunk:
    def __init__(self, cx: int, cz: int):
        self.coord = (cx, cz)
        self.blocks = np.zeros((config.CHUNK_SIZE, config.CHUNK_HEIGHT, config.CHUNK_SIZE), dtype=np.uint8)
        self.entities = []
        self.is_dirty = True

class PhysicsObject:
    def __init__(self, pos: np.ndarray, size: float = 1.0, mass: float = 100.0):
        self.pos = pos.astype(float)
        self.vel = np.zeros(3, dtype=float)
        self.acc = np.zeros(3, dtype=float)
        self.size = size
        self.mass = mass
        self.on_ground = False
        self.friction_factor = 0.9
        self.fatigue_level = 0.0  # Embodiment #18

class QuantumObject(PhysicsObject):
    def __init__(self, pos: np.ndarray, size: float = 1.0, mass: float = 1.0):
        super().__init__(pos, size, mass)
        self.superposition_states: Dict[str, float] = {'active': 0.5, 'inert': 0.5}
        self.is_collapsed = False
        self.entangled_with = None  # Embodiment #11
        self.coherence = 1.0

    def collapse(self, state: str):
        """Quantum-accurate wave function collapse"""
        self.is_collapsed = True
        self.superposition_states = {s: (1.0 if s == state else 0.0) for s in self.superposition_states}
        logger.info(f"Quantum Object collapsed to state: {state}")
        
        # Educational note: This represents measurement in quantum mechanics
        print("  [QUANTUM LESSON] Measurement collapses superposition to definite state!")

    def entangle(self, other: 'QuantumObject'):
        """Create quantum entanglement between two objects"""
        self.entangled_with = other
        other.entangled_with = self
        logger.info("Quantum entanglement established!")
        print("  [QUANTUM LESSON] Entanglement: Objects share fate across any distance!")

# === QUANTUM PROCESSOR (From Nexus Core) ===
class QuantumProcessor:
    """Accurate quantum computing simulation for educational purposes"""
    
    def __init__(self):
        self.qubits = []
        self.gate_operations = []
        
    def hadamard(self, qubit_idx: int):
        """Apply Hadamard gate to create superposition"""
        logger.info(f"Applied Hadamard gate to qubit {qubit_idx}")
        print("  [QUANTUM LESSON] Hadamard creates equal superposition |0> + |1>!")
        
    def measure(self, qubit_idx: int) -> int:
        """Quantum measurement collapses superposition"""
        result = random.randint(0, 1)
        logger.info(f"Measurement result for qubit {qubit_idx}: {result}")
        return result

quantum_processor = QuantumProcessor()

# === COGNITIVE ARCHITECTURE ===
class CognitiveArchitecture:
    """Implements cognitive science principles including bounded rationality"""
    
    def __init__(self):
        self.working_memory = deque(maxlen=config.WORKING_MEMORY_LIMIT)
        self.long_term_memory = {}
        self.affective_state = "neutral"  # Emotional context
        self.curiosity_level = 0.5
        
    def process_sensory_input(self, sensory_data: Dict) -> Dict:
        """Apply Miller's Law and cognitive principles"""
        # Keep only most recent sensory inputs within working memory limit
        self.working_memory.append(sensory_data)
        
        # Compress for long-term storage (simplified)
        if len(self.working_memory) >= config.WORKING_MEMORY_LIMIT:
            key = f"memory_{len(self.long_term_memory)}"
            self.long_term_memory[key] = list(self.working_memory)[-3:]  # Keep recent
        
        return {"processed": sensory_data, "affective": self.affective_state}

# === EMBODIMENT UPGRADES SYSTEM ===
class EmbodimentUpgrade(Enum):
    HAPTIC_SENSES = 1
    ECHO_LOCATION = 2
    OLFACTORY_SENSES = 3
    THERMAL_SENSES = 4
    PROPRIOCEPTIVE = 5
    MULTIVERSE_ECHO = 6
    PROBABILISTIC_LIMB = 7
    RESONANCE_PROPULSION = 8
    ADAPTIVE_MORPHOLOGY = 9
    ECHO_MOTOR_LOOP = 10
    ENTANGLEMENT_PAIRING = 11
    INERTIAL_DRIFT = 12
    CURIOSITY_DRIVE = 13
    FAILURE_REPLAY = 14
    CROSS_MODAL_TRANSFER = 15
    SOCIAL_MIRRORING = 16
    FATIGUE_CYCLES = 17
    PREDICTIVE_SIM = 18
    VOXEL_SYMBiosis = 19
    TEMPORAL_LAYERS = 20
    SCALE_INVARIANT = 21
    EMOTIONAL_FIELDS = 22
    HOLOGRAPHIC_PROJECTION = 23
    ECO_FEEDBACK = 24

class EmbodimentManager:
    """Manages unlockable embodiment upgrades"""
    
    def __init__(self):
        self.unlocked_upgrades = set()
        self.available_points = 0
        
    def unlock(self, upgrade: EmbodimentUpgrade) -> bool:
        if upgrade in self.unlocked_upgrades:
            return False
            
        if self.available_points >= config.EMBODIMENT_UNLOCK_COST:
            self.unlocked_upgrades.add(upgrade)
            self.available_points -= config.EMBODIMENT_UNLOCK_COST
            logger.info(f"Embodiment upgrade unlocked: {upgrade.name}")
            
            # Fun unlock messages
            unlock_messages = {
                EmbodimentUpgrade.HAPTIC_SENSES: "Your fingertips tingle with quantum vibrations!",
                EmbodimentUpgrade.INERTIAL_DRIFT: "Gravity becomes your playground!",
                EmbodimentUpgrade.CURIOSITY_DRIVE: "The universe wonders with you!",
                EmbodimentUpgrade.ENTANGLEMENT_PAIRING: "You're never alone in the multiverse!",
            }
            if upgrade in unlock_messages:
                print(f"  [UPGRADE] {unlock_messages[upgrade]}")
                
            return True
        return False
        
    def add_points(self, points: int):
        self.available_points += points

# === NOVEL PHYSICS ENGINES ===
class NeuralPhysicsPredictor:
    """AI model for complex physics prediction"""
    def predict_collision_outcome(self, obj1: PhysicsObject, obj2: PhysicsObject) -> Dict:
        # Simple elastic collision fallback
        return {'obj1_vel': obj1.vel * -0.8, 'obj2_vel': obj2.vel * -0.8}

class MolecularDynamicsEngine:
    """Molecular interactions with scent fields (Embodiment #3)"""
    def __init__(self):
        self.molecules = []
        self.scent_field = {}  # Position -> scent intensity
        
    def update(self, dt: float):
        # Placeholder for molecular dynamics
        pass
        
    def get_scent_gradient(self, pos: np.ndarray) -> np.ndarray:
        """Olfactory sensing (Embodiment #3)"""
        if EmbodimentUpgrade.OLFACTORY_SENSES in embodiment_manager.unlocked_upgrades:
            # Simple gradient towards strongest scent
            max_scent_pos = None
            max_scent = 0
            for scent_pos, intensity in self.scent_field.items():
                if intensity > max_scent:
                    max_scent = intensity
                    max_scent_pos = scent_pos
                    
            if max_scent_pos is not None:
                direction = max_scent_pos - pos
                return direction / (np.linalg.norm(direction) + 1e-6)
        return np.zeros(3)

class QuantumFluidDynamics:
    """Quantum fluid simulation with echo-location (Embodiment #2)"""
    def __init__(self, grid_size=(16, 16, 16)):
        self.grid_size = grid_size
        self.velocity_field = np.zeros(grid_size + (3,), dtype=float)
        self.echo_map = {}  # For echo-location
        
    def update(self, dt: float):
        # Placeholder for quantum fluid dynamics
        pass
        
    def get_echo_data(self, pos: np.ndarray) -> Dict:
        """Echo-location sensing (Embodiment #2)"""
        if EmbodimentUpgrade.ECHO_LOCATION in embodiment_manager.unlocked_upgrades:
            return {"nearby_objects": 3, "distance": 5.0}  # Stub
        return {}

# === AGI SENSORY/MOTOR INTERFACE WITH EMBODIMENT ===
class AGISensoryInterface:
    """Enhanced sensory system with embodiment upgrades"""
    
    def __init__(self, world: 'WorldState', agent_id: str):
        self.world = world
        self.agent_id = agent_id
        self.thermal_map = {}  # Embodiment #4
        
    def get_local_voxels(self, radius: int = 5) -> np.ndarray:
        """Voxel vision with haptic enhancement (Embodiment #1)"""
        agent = self.world.entities.get(self.agent_id)
        if not agent: 
            return np.zeros((1, 1, 1))

        # Haptic sensing enhancement
        if EmbodimentUpgrade.HAPTIC_SENSES in embodiment_manager.unlocked_upgrades:
            radius += 2  # Extended range
            
        x, y, z = [int(p // 1) for p in agent.pos]
        view_size = radius * 2 + 1
        
        # Create local view
        local_view = np.zeros((view_size, view_size, view_size), dtype=np.uint8)
        
        for dx in range(-radius, radius + 1):
            for dy in range(-radius, radius + 1):
                for dz in range(-radius, radius + 1):
                    wx, wy, wz = x + dx, y + dy, z + dz
                    block_type = self.world.get_block(wx, wy, wz)
                    local_view[dx + radius, dy + radius, dz + radius] = block_type.value
                    
        return local_view

    def get_entity_list(self) -> List[Dict]:
        """Entity detection with social mirroring (Embodiment #16)"""
        agent = self.world.entities.get(self.agent_id)
        if not agent: 
            return []
            
        perceived_entities = []
        for entity_id, entity in self.world.entities.items():
            if entity_id != self.agent_id:
                distance = np.linalg.norm(entity.pos - agent.pos)
                
                entity_data = {
                    'id': entity_id,
                    'type': entity.__class__.__name__,
                    'distance': distance,
                    'velocity': entity.vel.tolist()
                }
                
                # Social mirroring enhancement
                if (EmbodimentUpgrade.SOCIAL_MIRRORING in embodiment_manager.unlocked_upgrades and 
                    distance < 10.0):
                    entity_data['mirrored_action'] = 'move'  # Stub for mirroring
                    
                perceived_entities.append(entity_data)
                
        return perceived_entities
        
    def get_temperature(self, pos: np.ndarray) -> float:
        """Thermal sensing (Embodiment #4)"""
        if EmbodimentUpgrade.THERMAL_SENSES in embodiment_manager.unlocked_upgrades:
            # Simple temperature based on height and block type
            base_temp = 20.0  # Base temperature
            height_factor = -0.1 * pos[1]  # Colder at lower heights
            return base_temp + height_factor
        return 20.0

class AGIMotorInterface:
    """Enhanced motor system with embodiment upgrades"""
    
    def __init__(self, world: 'WorldState', agent_id: str):
        self.world = world
        self.agent_id = agent_id
        self.move_speed = 10.0
        self.jump_force = 20.0
        self.resonance_charge = 0.0  # Embodiment #8
        
    def apply_action(self, action: str, direction: np.ndarray = np.array([0, 0, 0])):
        """Apply motor actions with embodiment enhancements"""
        agent = self.world.entities.get(self.agent_id)
        if not agent: 
            return

        if action == 'move':
            # Direction should be a normalized 3D vector
            move_power = self.move_speed
            
            # Inertial drift enhancement (Embodiment #12)
            if EmbodimentUpgrade.INERTIAL_DRIFT in embodiment_manager.unlocked_upgrades:
                # Convert falling velocity into horizontal boost
                if agent.vel[1] < -5.0:
                    horizontal_boost = -agent.vel[1] * 0.1
                    move_power += horizontal_boost
                    logger.info("Inertial drift engaged!")
                    
            agent.acc += direction * move_power
            
        elif action == 'jump' and agent.on_ground:
            agent.vel[1] = self.jump_force
            agent.on_ground = False
            
        elif action == 'interact':
            # Quantum interaction
            self._handle_quantum_interaction(agent)
            
        elif action == 'rest':
            agent.acc = np.zeros(3)
            agent.fatigue_level = max(0.0, agent.fatigue_level - 0.1)  # Embodiment #17
            
        elif action == 'resonate' and EmbodimentUpgrade.RESONANCE_PROPULSION in embodiment_manager.unlocked_upgrades:
            # Resonance propulsion (Embodiment #8)
            self.resonance_charge += 0.1
            if self.resonance_charge >= 1.0:
                agent.vel += direction * 15.0
                self.resonance_charge = 0.0
                logger.info("Resonance propulsion activated!")
                
    def _handle_quantum_interaction(self, agent: PhysicsObject):
        """Handle quantum-specific interactions"""
        # Look for nearby quantum objects
        for entity_id, entity in self.world.entities.items():
            if entity_id != self.agent_id and isinstance(entity, QuantumObject):
                distance = np.linalg.norm(entity.pos - agent.pos)
                if distance < 3.0:
                    # Collapse or entangle
                    if random.random() > 0.5:
                        entity.collapse('active')
                        embodiment_manager.add_points(10)  # Reward for interaction
                    else:
                        # Try to entangle with agent if it's quantum
                        if isinstance(agent, QuantumObject):
                            entity.entangle(agent)
                    break

# === QUANTUM REINFORCEMENT LEARNING WITH CURIOSITY ===
class QuantumReinforcementLearning:
    """QRL with curiosity drive and cognitive enhancements"""
    
    def __init__(self):
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.epsilon = 0.1  # Exploration rate
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.curiosity_bonus = config.CURIOSITY_BONUS
        
    def get_action(self, sensory_input: Dict[str, Any]) -> Tuple[str, Optional[np.ndarray]]:
        """Get action with curiosity drive (Embodiment #13)"""
        
        state_key = self._state_to_key(sensory_input)
        
        # Curiosity-driven exploration
        if (EmbodimentUpgrade.CURIOSITY_DRIVE in embodiment_manager.unlocked_upgrades and 
            random.random() < self.curiosity_bonus):
            return self._get_curious_action()
        
        # Epsilon-greedy policy
        if random.random() < self.epsilon:
            return self._get_random_action()
            
        # Choose best action from Q-table
        best_action = None
        best_value = -float('inf')
        
        for action in ['move', 'jump', 'interact', 'rest']:
            if self.q_table[state_key][action] > best_value:
                best_value = self.q_table[state_key][action]
                best_action = action
                
        if best_action == 'move':
            direction = self._get_smart_direction(sensory_input)
            return 'move', direction
        elif best_action == 'jump':
            return 'jump', None
        elif best_action == 'interact':
            return 'interact', None
        else:
            return 'rest', None
            
    def _get_curious_action(self) -> Tuple[str, Optional[np.ndarray]]:
        """Curiosity-driven action selection"""
        logger.info("Curiosity drive activated!")
        actions = ['move', 'jump', 'interact']
        action = random.choice(actions)
        
        if action == 'move':
            direction = np.array([random.uniform(-1, 1), 0, random.uniform(-1, 1)])
            direction /= np.linalg.norm(direction) if np.linalg.norm(direction) > 0 else 1.0
            return 'move', direction
        return action, None
        
    def _get_random_action(self) -> Tuple[str, Optional[np.ndarray]]:
        """Random action for exploration"""
        actions = ['move', 'jump', 'interact', 'rest']
        action = random.choice(actions)
        
        if action == 'move':
            direction = np.array([random.uniform(-1, 1), 0, random.uniform(-1, 1)])
            direction /= np.linalg.norm(direction) if np.linalg.norm(direction) > 0 else 1.0
            return 'move', direction
        return action, None
        
    def _get_smart_direction(self, sensory_input: Dict) -> np.ndarray:
        """Get direction toward interesting features"""
        # Simple heuristic: move toward quantum ores
        if sensory_input.get('quantum_ore_nearby', False):
            return np.array([0, 0, 1])  # Move forward (stub)
        return np.array([random.uniform(-1, 1), 0, random.uniform(-1, 1)])
        
    def _state_to_key(self, sensory_input: Dict) -> str:
        """Convert sensory input to state key"""
        return f"{sensory_input.get('local_voxels_mean', 0):.2f}_{sensory_input.get('num_nearby_entities', 0)}"
        
    def update_q_value(self, state: str, action: str, reward: float, next_state: str):
        """Standard Q-learning update"""
        best_next_value = max(self.q_table[next_state].values()) if self.q_table[next_state] else 0
        current_value = self.q_table[state][action]
        
        new_value = current_value + self.learning_rate * (
            reward + self.discount_factor * best_next_value - current_value
        )
        self.q_table[state][action] = new_value

# === WORLD GENERATION ===
class QuantumProceduralGenerator:
    """Quantum-inspired world generation with symbiosis (Embodiment #19)"""
    
    def __init__(self, seed: Optional[int] = None):
        self.seed = seed if seed is not None else random.randint(0, 2**32 - 1)
        np.random.seed(self.seed)
        self.symbiosis_bias = 0.0  # Embodiment #19
        
    def generate_chunk(self, cx: int, cz: int) -> Chunk:
        """Generate chunk with quantum-inspired terrain"""
        chunk = Chunk(cx, cz)
        
        # Vectorized terrain generation
        X, Z = np.meshgrid(np.arange(config.CHUNK_SIZE), np.arange(config.CHUNK_SIZE))
        X += cx * config.CHUNK_SIZE
        Z += cz * config.CHUNK_SIZE
        
        # Quantum noise with symbiosis bias
        noise = (np.sin(X * 0.1) + np.cos(Z * 0.1) + 
                np.random.rand(config.CHUNK_SIZE, config.CHUNK_SIZE) * 0.5) / 3.0
        
        # Apply symbiosis bias (Embodiment #19)
        if EmbodimentUpgrade.VOXEL_SYMBiosis in embodiment_manager.unlocked_upgrades:
            noise += self.symbiosis_bias * 0.1
            
        heightmap = config.TERRAIN_BASE_HEIGHT + (noise * config.HILL_AMPLITUDE).astype(int)
        
        # Fill chunk
        for x in range(config.CHUNK_SIZE):
            for z in range(config.CHUNK_SIZE):
                h = heightmap[x, z]
                for y in range(config.CHUNK_HEIGHT):
                    if y < 1:  # Bedrock layer
                        chunk.blocks[x, y, z] = BlockType.BEDROCK.value
                    elif y < h - 4:
                        chunk.blocks[x, y, z] = BlockType.STONE.value
                    elif y < h:
                        chunk.blocks[x, y, z] = BlockType.DIRT.value
                    elif y == h:
                        chunk.blocks[x, y, z] = BlockType.GRASS.value
                    
                    # Quantum ore generation
                    if (random.random() < 0.001 and 
                        y < config.TERRAIN_BASE_HEIGHT // 2 and 
                        y > 10):  # Avoid bedrock level
                        chunk.blocks[x, y, z] = BlockType.QUANTUM_ORE.value
                         
        logger.info(f"Generated chunk {chunk.coord}")
        return chunk

# === QUANTUM AND MULTIVERSE SYSTEMS ===
class SchrodingerItem:
    """Quantum item that collapses on observation"""
    
    def __init__(self, item_id: str, possible_states: List[Dict[str, Any]]):
        self.item_id = item_id
        self.possible_states = possible_states
        self.is_collapsed = False
        self.final_state: Dict[str, Any] = {}

    def observe(self) -> Dict[str, Any]:
        """Collapse wave function through observation"""
        if not self.is_collapsed:
            self.final_state = random.choice(self.possible_states)
            self.is_collapsed = True
            logger.info(f"SchrÃ¶dinger Item collapsed to: {self.final_state}")
            
            # Educational note
            print("  [QUANTUM LESSON] SchrÃ¶dinger's Cat: Both alive and dead until observed!")
            
        return self.final_state

class TimelineManager:
    """Multiverse timeline management"""
    
    def __init__(self, initial_world_state: 'WorldState'):
        self.timelines: Dict[str, 'WorldState'] = {"main": initial_world_state}
        self.current_timeline_id = "main"
        self.fork_count = 0
        
    def fork_reality(self, choice: str) -> bool:
        """Fork current timeline (limited by game balance)"""
        if self.fork_count >= config.MAX_TIMELINES:
            logger.warning("Maximum timeline count reached!")
            return False
            
        new_timeline_id = f"{self.current_timeline_id}-{choice}_{random.randint(100, 999)}"
        
        # In a real implementation, this would copy world state
        # For now, we'll just track the fork
        self.timelines[new_timeline_id] = self.timelines[self.current_timeline_id]  # Reference only
        self.current_timeline_id = new_timeline_id
        self.fork_count += 1
        
        logger.info(f"Reality forked! New timeline: {new_timeline_id}")
        print("  [MULTIVERSE] You've created a new branch in the quantum tree!")
        
        return True

class QuantumPlayerModel:
    """Predictive model for agent movement"""
    
    def __init__(self):
        self.position_history: List[np.ndarray] = []
        self.velocity: np.ndarray = np.zeros(3)

    def update(self, agent_pos: np.ndarray, dt: float):
        """Update predictive model"""
        if len(self.position_history) > 0:
            current_vel = (agent_pos - self.position_history[-1]) / (dt + 1e-6)
            self.velocity = self.velocity * 0.9 + current_vel * 0.1  # EMA smoothing
            
        if len(self.position_history) >= 10:
            self.position_history.pop(0)
        self.position_history.append(agent_pos.copy())

    def get_chunk_load_probabilities(self, current_chunk: Tuple[int, int]) -> Dict[Tuple[int, int], float]:
        """Predict chunk loading probabilities"""
        x, z = current_chunk
        probabilities = {
            (x, z): 0.6,
            (x, z + 1): 0.2,
            (x, z - 1): 0.05,
            (x + 1, z): 0.05,
            (x - 1, z): 0.05,
            (x + 1, z + 1): 0.05
        }
        return probabilities

# === WORLD STATE AND PHYSICS ENGINE ===
class PhysicsEngine:
    """Enhanced physics with raycast collisions and embodiment"""
    
    def __init__(self, world: 'WorldState'):
        self.world = world

    def update(self, dt: float):
        """Physics update with sub-stepping for stability"""
        sub_dt = dt / config.COLLISION_SUB_STEPS
        
        for _ in range(config.COLLISION_SUB_STEPS):
            for entity in self.world.entities.values():
                if not isinstance(entity, PhysicsObject): 
                    continue

                # Apply gravity
                entity.acc[1] += config.GRAVITY

                # Apply acceleration to velocity
                entity.vel += entity.acc * sub_dt
                
                # Velocity clamping
                entity.vel = np.clip(entity.vel, -config.MAX_VELOCITY, config.MAX_VELOCITY)
                
                # Apply friction if on ground
                if entity.on_ground and np.linalg.norm(entity.acc) < 0.1:
                     entity.vel[0] *= entity.friction_factor
                     entity.vel[2] *= entity.friction_factor

                # Update position with collision resolution
                new_pos = entity.pos + entity.vel * sub_dt
                self.resolve_collisions_raycast(entity, new_pos, sub_dt)

                # Reset acceleration
                entity.acc = np.zeros(3)
                
                # Fatigue system (Embodiment #17)
                if isinstance(entity, AGI_Agent):
                    movement = np.linalg.norm(entity.vel) * sub_dt
                    entity.fatigue_level = min(1.0, entity.fatigue_level + movement * 0.01)

    def resolve_collisions_raycast(self, entity: PhysicsObject, new_pos: np.ndarray, dt: float):
        """Raycast-based collision detection to prevent tunneling"""
        
        # Simple raycast along velocity direction
        if np.linalg.norm(entity.vel) > 0.1:
            direction = entity.vel / np.linalg.norm(entity.vel)
            check_distance = np.linalg.norm(entity.vel * dt) + entity.size
            
            # Check multiple points along the movement path
            steps = max(2, int(check_distance * 2))
            for i in range(steps):
                t = (i + 1) / steps
                check_pos = entity.pos + direction * check_distance * t
                
                # Check collision at this point
                if self.check_voxel_collision(entity, check_pos):
                    # Collision found - push back and stop velocity
                    push_back_distance = check_distance * (i / steps)
                    safe_pos = entity.pos + direction * push_back_distance
                    
                    # Determine collision normal and respond
                    entity.pos = safe_pos
                    
                    # Stop velocity in collision direction
                    entity.vel = np.zeros(3)  # Simplified response
                    entity.on_ground = True if direction[1] < 0 else False
                    return
                    
        # No collision detected, move to new position
        entity.pos = new_pos
        entity.on_ground = False

    def check_voxel_collision(self, entity: PhysicsObject, pos: np.ndarray) -> bool:
        """Check if entity collides with voxels at given position"""
        min_x, max_x = int(pos[0] - entity.size/2), int(pos[0] + entity.size/2) + 1
        min_y, max_y = int(pos[1] - entity.size/2), int(pos[1] + entity.size/2) + 1
        min_z, max_z = int(pos[2] - entity.size/2), int(pos[2] + entity.size/2) + 1

        for x in range(min_x, max_x):
            for y in range(min_y, max_y):
                for z in range(min_z, max_z):
                    block_type = self.world.get_block(x, y, z)
                    if block_type not in [BlockType.AIR, BlockType.WATER]:
                        return True
        return False

class WorldState:
    """Complete world simulation state"""
    
    def __init__(self):
        self.chunks: Dict[Tuple[int, int], Chunk] = {}
        self.entities: Dict[str, PhysicsObject] = {}
        self.physics_engine = PhysicsEngine(self)
        self.md_engine = MolecularDynamicsEngine()
        self.qfd_engine = QuantumFluidDynamics()
        self.neural_predictor = NeuralPhysicsPredictor()
        self.quantum_ores_collected = 0
        self.entropy_level = 0.0

    def get_or_create_chunk(self, cx: int, cz: int) -> Chunk:
        """Get or generate chunk"""
        coord = (cx, cz)
        if coord not in self.chunks:
            self.chunks[coord] = self.generator.generate_chunk(cx, cz)
        return self.chunks[coord]

    def get_block(self, x: int, y: int, z: int) -> BlockType:
        """Get block type at world coordinates with bedrock safety"""
        # Bedrock at bottom and top of world
        if y < 0 or y >= config.CHUNK_HEIGHT:
            return BlockType.BEDROCK
            
        cx, cz = x // config.CHUNK_SIZE, z // config.CHUNK_SIZE
        lx, lz = x % config.CHUNK_SIZE, z % config.CHUNK_SIZE
        
        chunk = self.chunks.get((cx, cz))
        if chunk is None:
            # Return stone below base height, air above
            return BlockType.STONE if y < config.TERRAIN_BASE_HEIGHT else BlockType.AIR
            
        block_value = chunk.blocks[lx, y, lz]
        return BlockType(block_value)

    def step_simulation(self, dt: float):
        """Main simulation update"""
        # Classical physics
        self.physics_engine.update(dt)
        
        # Novel physics
        self.md_engine.update(dt)
        self.qfd_engine.update(dt)
        
        # Quantum entity updates
        for entity in self.entities.values():
            if isinstance(entity, QuantumObject) and not entity.is_collapsed:
                # Quantum decoherence
                for state in entity.superposition_states:
                    entity.superposition_states[state] += (random.random() - 0.5) * dt * config.SUPERPOSITION_DECOHERENCE_RATE
                
                # Normalize probabilities
                total = sum(entity.superposition_states.values())
                for state in entity.superposition_states:
                    entity.superposition_states[state] /= total
                    
                # Entanglement effects (Embodiment #11)
                if (entity.entangled_with and 
                    EmbodimentUpgrade.ENTANGLEMENT_PAIRING in embodiment_manager.unlocked_upgrades):
                    # Sync states with entangled partner
                    entity.superposition_states = entity.entangled_with.superposition_states.copy()

# === AGI AGENT ===
class AGI_Agent(QuantumObject):
    """The player's AGI entity with full cognitive architecture"""
    
    def __init__(self, pos: np.ndarray, world_state: WorldState):
        super().__init__(pos, size=0.8, mass=70.0)
        self.id = "agi_unit_01"
        self.qrl_agent = QuantumReinforcementLearning()
        self.cognitive_arch = CognitiveArchitecture()
        self.sensory_system = AGISensoryInterface(world_state, self.id)
        self.motor_system = AGIMotorInterface(world_state, self.id)
        self.quantum_player_model = QuantumPlayerModel()
        self.score = 0
        self.quantum_ores_collected = 0
        
        logger.info(f"AGI Agent '{self.id}' initialized")

    def think(self):
        """Complete cognitive cycle with embodiment enhancements"""
        # 1. Gather sensory input
        voxel_data = self.sensory_system.get_local_voxels()
        entity_list = self.sensory_system.get_entity_list()
        temperature = self.sensory_system.get_temperature(self.pos)
        
        # Check for quantum ore in local view
        quantum_ore_nearby = np.any(voxel_data == BlockType.QUANTUM_ORE.value)
        
        # 2. Cognitive processing
        sensory_input = {
            'local_voxels_mean': np.mean(voxel_data),
            'num_nearby_entities': len(entity_list),
            'distance_to_nearest_entity': min([e['distance'] for e in entity_list]) if entity_list else 100.0,
            'quantum_ore_nearby': quantum_ore_nearby,
            'temperature': temperature,
            'fatigue_level': self.fatigue_level
        }
        
        processed_input = self.cognitive_arch.process_sensory_input(sensory_input)
        
        # 3. Get action from QRL (with fatigue influence)
        if self.fatigue_level > config.FATIGUE_THRESHOLD:
            # Force rest when fatigued
            action, direction = 'rest', None
            logger.info("Fatigue high - forcing rest")
        else:
            action, direction = self.qrl_agent.get_action(processed_input)
        
        # 4. Apply motor action
        self.motor_system.apply_action(action, direction)
        
        # 5. Update predictive model
        self.quantum_player_model.update(self.pos, 1.0/60.0)
        
        # 6. Learn from experience
        self._learn_from_experience(sensory_input, action)
        
    def _learn_from_experience(self, sensory_input: Dict, action: str):
        """Reinforcement learning update"""
        # Simple reward based on discoveries and survival
        reward = 0.0
        
        if sensory_input['quantum_ore_nearby']:
            reward += 5.0
            
        if self.on_ground and not sensory_input['quantum_ore_nearby']:
            reward += 0.1  # Small reward for safe exploration
            
        if self.fatigue_level > 0.8:
            reward -= 1.0  # Penalize over-exertion
            
        # Update Q-learning (simplified)
        state_key = self.qrl_agent._state_to_key(sensory_input)
        next_state_key = state_key  # In reality, this would be the next state
        self.qrl_agent.update_q_value(state_key, action, reward, next_state_key)

# === GAME MANAGER ===
class GameManager:
    """Main game controller with quests and UI"""
    
    def __init__(self):
        self.world = WorldState()
        self.generator = QuantumProceduralGenerator(seed=42)
        self.world.generator = self.generator
        
        # Create AGI agent
        initial_pos = np.array([0.0, config.TERRAIN_BASE_HEIGHT + 2.0, 0.0])
        self.agi_agent = AGI_Agent(initial_pos, self.world)
        self.world.entities[self.agi_agent.id] = self.agi_agent
        
        # Game systems
        self.timeline_manager = TimelineManager(self.world)
        self.is_running = True
        self.game_tick = 0
        self.quests = {
            "collect_ores": {"target": 5, "progress": 0, "reward": 50, "description": "Collect 5 Quantum Ores"},
            "stabilize_timelines": {"target": 3, "progress": 0, "reward": 100, "description": "Stabilize 3 Timelines"},
            "master_embodiment": {"target": 5, "progress": 0, "reward": 200, "description": "Unlock 5 Embodiment Upgrades"}
        }
        
        # Generate initial chunks
        cx, cz = int(initial_pos[0] // config.CHUNK_SIZE), int(initial_pos[2] // config.CHUNK_SIZE)
        self._generate_initial_chunks(cx, cz)
        
        logger.info("=== QUANTUM EMBODIMENT QUEST INITIALIZED ===")
        print("\nWelcome to Quantum Embodiment Quest!")
        print("Your mission: Explore the quantum-voxel world, collect Quantum Ores,")
        print("stabilize timelines, and master embodiment upgrades!")
        print("\nCommands: move [direction], jump, interact, rest, fork, upgrade, status, quit")
        
    def _generate_initial_chunks(self, center_x: int, center_z: int):
        """Generate initial chunks around player"""
        for x in range(center_x - config.VIEW_DISTANCE, center_x + config.VIEW_DISTANCE + 1):
            for z in range(center_z - config.VIEW_DISTANCE, center_z + config.VIEW_DISTANCE + 1):
                coord = (x, z)
                if coord not in self.world.chunks:
                    self.world.chunks[coord] = self.generator.generate_chunk(x, z)

    def display_world_ascii(self):
        """Simple ASCII representation of the world around the agent"""
        print("\n" + "="*50)
        print("QUANTUM WORLD VIEW")
        print("="*50)
        
        agent_pos = self.agi_agent.pos
        print(f"Position: ({agent_pos[0]:.1f}, {agent_pos[1]:.1f}, {agent_pos[2]:.1f})")
        print(f"Velocity: {np.linalg.norm(self.agi_agent.vel):.1f} m/s")
        print(f"Timeline: {self.timeline_manager.current_timeline_id}")
        print(f"Quantum Ores: {self.agi_agent.quantum_ores_collected}/{config.QUANTUM_ORES_TO_WIN}")
        print(f"Embodiment Points: {embodiment_manager.available_points}")
        print(f"Unlocked Upgrades: {len(embodiment_manager.unlocked_upgrades)}")
        
        # Simple top-down view
        print("\nTop-Down View:")
        size = 5
        center_x, center_z = int(agent_pos[0]), int(agent_pos[2])
        
        for z in range(center_z - size, center_z + size + 1):
            line = ""
            for x in range(center_x - size, center_x + size + 1):
                if x == center_x and z == center_z:
                    line += "A"  # Agent
                else:
                    block = self.world.get_block(x, int(agent_pos[1]), z)
                    if block == BlockType.QUANTUM_ORE:
                        line += "*"  # Quantum ore
                    elif block != BlockType.AIR:
                        line += "#"  # Solid block
                    else:
                        line += "."  # Air
            print(line)
            
        print("Legend: A=You, *=Quantum Ore, #=Solid, .=Air")
        print("="*50)

    def process_command(self, command: str) -> bool:
        """Process player command"""
        parts = command.lower().split()
        if not parts:
            return True
            
        cmd = parts[0]
        
        if cmd == "move" and len(parts) > 1:
            direction_map = {
                "north": np.array([0, 0, 1]),
                "south": np.array([0, 0, -1]),
                "east": np.array([1, 0, 0]),
                "west": np.array([-1, 0, 0]),
                "up": np.array([0, 1, 0]),
                "down": np.array([0, -1, 0])
            }
            
            direction = parts[1]
            if direction in direction_map:
                self.agi_agent.motor_system.apply_action('move', direction_map[direction])
                print(f"Moving {direction}")
            else:
                print("Invalid direction. Use: north, south, east, west, up, down")
                
        elif cmd == "jump":
            self.agi_agent.motor_system.apply_action('jump')
            print("Jumping!")
            
        elif cmd == "interact":
            self.agi_agent.motor_system.apply_action('interact')
            print("Interacting with environment...")
            
        elif cmd == "rest":
            self.agi_agent.motor_system.apply_action('rest')
            print("Resting...")
            
        elif cmd == "fork":
            if self.timeline_manager.fork_reality("player_choice"):
                print("Timeline forked! The multiverse expands...")
            else:
                print("Cannot fork timeline - maximum reached or insufficient resources")
                
        elif cmd == "upgrade":
            if len(parts) > 1:
                self._handle_upgrade(parts[1])
            else:
                self._show_available_upgrades()
                
        elif cmd == "status":
            self.display_world_ascii()
            
        elif cmd == "quit":
            print("Thanks for playing Quantum Embodiment Quest!")
            return False
            
        else:
            print("Unknown command. Available: move, jump, interact, rest, fork, upgrade, status, quit")
            
        return True
        
    def _handle_upgrade(self, upgrade_name: str):
        """Handle embodiment upgrade request"""
        try:
            upgrade_num = int(upgrade_name)
            upgrade = EmbodimentUpgrade(upgrade_num)
            
            if embodiment_manager.unlock(upgrade):
                print(f"Successfully unlocked {upgrade.name}!")
                
                # Apply upgrade effects
                if upgrade == EmbodimentUpgrade.INERTIAL_DRIFT:
                    print("  You can now convert falling velocity into horizontal movement!")
                elif upgrade == EmbodimentUpgrade.CURIOSITY_DRIVE:
                    print("  Your exploration is now driven by quantum curiosity!")
                elif upgrade == EmbodimentUpgrade.HAPTIC_SENSES:
                    print("  Your senses now extend further into the quantum realm!")
                    
            else:
                print(f"Cannot unlock {upgrade.name}. Need {config.EMBODIMENT_UNLOCK_COST} points.")
                
        except (ValueError, ValueError):
            print("Invalid upgrade number. Use 'upgrade' to see available options.")
            
    def _show_available_upgrades(self):
        """Show available embodiment upgrades"""
        print("\nAvailable Embodiment Upgrades:")
        print("Points available:", embodiment_manager.available_points)
        print("Cost per upgrade:", config.EMBODIMENT_UNLOCK_COST)
        
        for upgrade in EmbodimentUpgrade:
            status = "UNLOCKED" if upgrade in embodiment_manager.unlocked_upgrades else "Available"
            print(f"  {upgrade.value}: {upgrade.name} - {status}")
            
        print("\nRecommended starters: 1 (Haptic), 7 (Probabilistic Limb), 12 (Inertial Drift), 13 (Curiosity)")

    def update(self, dt: float):
        """Game update loop"""
        self.game_tick += 1
        
        # AGI thinking
        self.agi_agent.think()
        
        # World simulation
        self.world.step_simulation(dt)
        
        # Check for quantum ore collection
        self._check_quantum_ore_interaction()
        
        # Check win/lose conditions
        if self._check_win_condition():
            self._handle_win()
            
        if self._check_lose_condition():
            self._handle_lose()
            
        # Quest updates
        self._update_quests()
        
        # Occasional quirky messages
        if random.random() < 0.01:  # 1% chance per update
            logger.quirky()

    def _check_quantum_ore_interaction(self):
        """Check if agent is interacting with quantum ore"""
        agent_pos = self.agi_agent.pos
        agent_chunk_x = int(agent_pos[0] // config.CHUNK_SIZE)
        agent_chunk_z = int(agent_pos[2] // config.CHUNK_SIZE)
        
        # Check nearby blocks for quantum ore
        for dx in range(-1, 2):
            for dy in range(-1, 2):
                for dz in range(-1, 2):
                    x, y, z = int(agent_pos[0]) + dx, int(agent_pos[1]) + dy, int(agent_pos[2]) + dz
                    block = self.world.get_block(x, y, z)
                    
                    if block == BlockType.QUANTUM_ORE:
                        # Collect ore
                        self.agi_agent.quantum_ores_collected += 1
                        self.world.quantum_ores_collected += 1
                        
                        # Replace with air
                        cx, cz = x // config.CHUNK_SIZE, z // config.CHUNK_SIZE
                        lx, lz = x % config.CHUNK_SIZE, z % config.CHUNK_SIZE
                        
                        if (cx, cz) in self.world.chunks:
                            self.world.chunks[(cx, cz)].blocks[lx, y, lz] = BlockType.AIR.value
                            
                        print(f"ðŸŽ‰ Quantum Ore collected! Total: {self.agi_agent.quantum_ores_collected}")
                        embodiment_manager.add_points(20)
                        
                        # Check quest
                        self.quests["collect_ores"]["progress"] += 1
                        return

    def _check_win_condition(self) -> bool:
        """Check if player has won"""
        return (self.agi_agent.quantum_ores_collected >= config.QUANTUM_ORES_TO_WIN and 
                self.timeline_manager.fork_count >= config.MAX_TIMELINES)

    def _check_lose_condition(self) -> bool:
        """Check if player has lost"""
        return (self.agi_agent.pos[1] < config.MAX_FALL_DISTANCE or 
                self.world.entropy_level > 1.0)

    def _handle_win(self):
        """Handle win condition"""
        print("\n" + "="*60)
        print("ðŸŽ‰ CONGRATULATIONS! YOU'VE WON QUANTUM EMBODIMENT QUEST!")
        print("="*60)
        print(f"You collected {self.agi_agent.quantum_ores_collected} Quantum Ores")
        print(f"You stabilized {self.timeline_manager.fork_count} timelines")
        print(f"You mastered {len(embodiment_manager.unlocked_upgrades)} embodiment upgrades")
        print("\nThe multiverse is stable! Reality thanks you!")
        self.is_running = False

    def _handle_lose(self):
        """Handle lose condition"""
        print("\n" + "="*60)
        print("ðŸ’€ GAME OVER - QUANTUM CATASTROPHE!")
        print("="*60)
        
        if self.agi_agent.pos[1] < config.MAX_FALL_DISTANCE:
            print("You fell into the quantum void!")
        else:
            print("Quantum entropy reached critical levels!")
            
        print("\nTry again with different embodiment strategies!")
        self.is_running = False

    def _update_quests(self):
        """Update quest progress and check completions"""
        # Update timeline quest
        self.quests["stabilize_timelines"]["progress"] = self.timeline_manager.fork_count
        
        # Update embodiment quest
        self.quests["master_embodiment"]["progress"] = len(embodiment_manager.unlocked_upgrades)
        
        # Check for completed quests
        for quest_name, quest_data in self.quests.items():
            if (quest_data["progress"] >= quest_data["target"] and 
                "completed" not in quest_data):
                
                print(f"\nðŸŽ¯ QUEST COMPLETED: {quest_data['description']}")
                print(f"Reward: {quest_data['reward']} embodiment points!")
                
                embodiment_manager.add_points(quest_data["reward"])
                quest_data["completed"] = True

# === GLOBAL INSTANCES ===
embodiment_manager = EmbodimentManager()

# === MAIN GAME LOOP ===
def main():
    """Main game loop"""
    game = GameManager()
    
    # Give starting points
    embodiment_manager.add_points(50)
    
    try:
        # Game loop
        while game.is_running:
            # Display status every 10 ticks
            if game.game_tick % 10 == 0:
                game.display_world_ascii()
                
            # Get player input
            try:
                command = input("\nCommand> ")
                if not game.process_command(command):
                    break
            except EOFError:
                print("\nGame interrupted")
                break
                
            # Game update
            game.update(1.0/60.0)  # 60 TPS
            
            # Brief pause to prevent CPU overuse
            time.sleep(1.0/60.0)
            
    except KeyboardInterrupt:
        print("\n\nGame interrupted by user")
    except Exception as e:
        logger.critical(f"Game crashed: {e}")
        import traceback
        traceback.print_exc()
        
    print("Thanks for playing Quantum Embodiment Quest!")

if __name__ == "__main__":
    main()
